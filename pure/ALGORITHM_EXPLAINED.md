# Giáº£i ThÃ­ch Chi Tiáº¿t Thuáº­t ToÃ¡n Panorama Stitching

TÃ i liá»‡u nÃ y giáº£i thÃ­ch chi tiáº¿t vá» cÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  triá»ƒn khai tá»«ng bÆ°á»›c cá»§a thuáº­t toÃ¡n ghÃ©p áº£nh panorama.

---

## ğŸ“š Má»¥c Lá»¥c

1. [Tá»•ng Quan Pipeline](#1-tá»•ng-quan-pipeline)
2. [SIFT - Scale-Invariant Feature Transform](#2-sift---scale-invariant-feature-transform)
3. [Feature Matching](#3-feature-matching)
4. [Homography Estimation & RANSAC](#4-homography-estimation--ransac)
5. [Image Warping](#5-image-warping)
6. [Weighted Blending](#6-weighted-blending)
7. [Tham Kháº£o](#7-tham-kháº£o)

---

## 1. Tá»•ng Quan Pipeline

### 1.1. Quy TrÃ¬nh Tá»•ng Thá»ƒ

```
Input Images
     â†“
[SIFT Feature Detection]
     â†“
Keypoints + Descriptors
     â†“
[Feature Matching]
     â†“
Match Pairs
     â†“
[RANSAC + Homography]
     â†“
Homography Matrix H
     â†“
[Image Warping]
     â†“
Warped Images
     â†“
[Weighted Blending]
     â†“
Panorama Output
```

### 1.2. File Cáº¥u TrÃºc

- **`sift.py`**: Triá»ƒn khai SIFT detector vÃ  descriptor
- **`matcher.py`**: Matching descriptors giá»¯a hai áº£nh
- **`homography.py`**: Æ¯á»›c lÆ°á»£ng homography vÃ  RANSAC
- **`blending.py`**: Blend áº£nh vá»›i weighted masks
- **`panorama_stitcher.py`**: Káº¿t há»£p táº¥t cáº£ thÃ nh pipeline hoÃ n chá»‰nh

---

## 2. SIFT - Scale-Invariant Feature Transform

### 2.1. CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t

SIFT (David Lowe, 1999) lÃ  thuáº­t toÃ¡n phÃ¡t hiá»‡n vÃ  mÃ´ táº£ Ä‘áº·c trÆ°ng báº¥t biáº¿n vá»›i:

- **Scale invariance**: Báº¥t biáº¿n vá»›i tá»· lá»‡ (zoom in/out)
- **Rotation invariance**: Báº¥t biáº¿n vá»›i xoay
- **Illumination invariance**: Báº¥t biáº¿n vá»›i thay Ä‘á»•i Ã¡nh sÃ¡ng

### 2.2. BÆ°á»›c 1: XÃ¢y Dá»±ng Scale Space

#### 2.2.1. Gaussian Pyramid

áº¢nh Ä‘Æ°á»£c lÃ m má» vá»›i Gaussian filters á»Ÿ nhiá»u scales khÃ¡c nhau:

```
L(x, y, Ïƒ) = G(x, y, Ïƒ) * I(x, y)
```

Trong Ä‘Ã³:

- `L(x, y, Ïƒ)`: Scale space representation
- `G(x, y, Ïƒ)`: Gaussian kernel vá»›i standard deviation Ïƒ
- `I(x, y)`: áº¢nh gá»‘c
- `*`: PhÃ©p convolution

**Gaussian Kernel:**

```
G(x, y, Ïƒ) = (1 / 2Ï€ÏƒÂ²) * exp(-(xÂ² + yÂ²) / 2ÏƒÂ²)
```

**Triá»ƒn Khai:**

```python
def _build_gaussian_pyramid(self, image):
    pyramid = []

    for octave in range(self.num_octaves):
        octave_pyramid = []

        # Downsample cho má»—i octave
        if octave == 0:
            base_image = image.copy()
        else:
            base_image = self._downsample(pyramid[octave - 1][-3])

        # Táº¡o scales trong octave
        for scale in range(self.num_scales + 3):
            sigma = sigma_0 * (k ** scale)  # k = 2^(1/num_scales)
            blurred = gaussian_filter(base_image, sigma)
            octave_pyramid.append(blurred)

        pyramid.append(octave_pyramid)

    return pyramid
```

**Táº¡i sao cáº§n nhiá»u octaves?**

- Má»—i octave xá»­ lÃ½ áº£nh á»Ÿ resolution khÃ¡c nhau
- Octave 0: áº¢nh gá»‘c
- Octave 1: áº¢nh downsampled 2x
- Octave 2: áº¢nh downsampled 4x
- ...

### 2.3. BÆ°á»›c 2: Difference of Gaussian (DoG)

DoG lÃ  xáº¥p xá»‰ cá»§a Laplacian of Gaussian, dÃ¹ng Ä‘á»ƒ phÃ¡t hiá»‡n blob:

```
D(x, y, Ïƒ) = L(x, y, kÏƒ) - L(x, y, Ïƒ)
```

**Táº¡i sao DoG?**

- Laplacian of Gaussian (LoG) tá»‘n kÃ©m tÃ­nh toÃ¡n
- DoG xáº¥p xá»‰ LoG nhÆ°ng nhanh hÆ¡n nhiá»u:

```
G(x, y, kÏƒ) - G(x, y, Ïƒ) â‰ˆ (k - 1)ÏƒÂ² âˆ‡Â²G
```

**Triá»ƒn Khai:**

```python
def _build_dog_pyramid(self, gaussian_pyramid):
    dog_pyramid = []

    for octave_pyramid in gaussian_pyramid:
        octave_dog = []
        for i in range(len(octave_pyramid) - 1):
            # Difference of Gaussians
            dog = octave_pyramid[i + 1] - octave_pyramid[i]
            octave_dog.append(dog)
        dog_pyramid.append(octave_dog)

    return dog_pyramid
```

### 2.4. BÆ°á»›c 3: PhÃ¡t Hiá»‡n Extrema

TÃ¬m local maxima vÃ  minima trong DoG scale space (3D: x, y, scale).

Má»™t Ä‘iá»ƒm lÃ  extrema náº¿u:

- Lá»›n hÆ¡n/nhá» hÆ¡n 26 Ä‘iá»ƒm lÃ¡ng giá»ng (8 trong cÃ¹ng scale + 9 á»Ÿ scale trÃªn + 9 á»Ÿ scale dÆ°á»›i)

```python
def _find_local_extrema(self, prev_dog, curr_dog, next_dog):
    # TÃ¬m maxima
    max_filter = maximum_filter(curr_dog, size=3)
    is_max = (curr_dog == max_filter)
    is_max &= (curr_dog > prev_dog)  # So vá»›i scale dÆ°á»›i
    is_max &= (curr_dog > next_dog)  # So vá»›i scale trÃªn
    is_max &= (abs(curr_dog) > self.contrast_threshold)

    # TÃ¬m minima
    min_filter = minimum_filter(curr_dog, size=3)
    is_min = (curr_dog == min_filter)
    is_min &= (curr_dog < prev_dog)
    is_min &= (curr_dog < next_dog)
    is_min &= (abs(curr_dog) > self.contrast_threshold)

    # Káº¿t há»£p
    is_extrema = is_max | is_min
    return np.argwhere(is_extrema)
```

### 2.5. BÆ°á»›c 4: Keypoint Refinement

#### 2.5.1. Sub-pixel Localization

Sá»­ dá»¥ng Taylor expansion Ä‘á»ƒ tÃ¬m vá»‹ trÃ­ chÃ­nh xÃ¡c hÆ¡n:

```
D(x) â‰ˆ D + (âˆ‚D/âˆ‚x)áµ€Â·x + (1/2)Â·xáµ€Â·(âˆ‚Â²D/âˆ‚xÂ²)Â·x
```

Äáº¡o hÃ m vÃ  tÃ¬m cá»±c trá»‹:

```
xÌ‚ = -(âˆ‚Â²D/âˆ‚xÂ²)â»Â¹ Â· (âˆ‚D/âˆ‚x)
```

Trong Ä‘Ã³ `x = [x, y, Ïƒ]áµ€`

**Triá»ƒn Khai:**

```python
def _refine_keypoint(self, octave_dog, scale_idx, y, x):
    # TÃ­nh gradient (Ä‘áº¡o hÃ m báº­c 1)
    dx = (curr_dog[y, x + 1] - curr_dog[y, x - 1]) / 2.0
    dy = (curr_dog[y + 1, x] - curr_dog[y - 1, x]) / 2.0
    ds = (next_dog[y, x] - prev_dog[y, x]) / 2.0

    # TÃ­nh Hessian (Ä‘áº¡o hÃ m báº­c 2)
    dxx = curr_dog[y, x + 1] + curr_dog[y, x - 1] - 2 * curr_dog[y, x]
    dyy = curr_dog[y + 1, x] + curr_dog[y - 1, x] - 2 * curr_dog[y, x]
    dss = next_dog[y, x] + prev_dog[y, x] - 2 * curr_dog[y, x]

    dxy = ((curr_dog[y + 1, x + 1] - curr_dog[y + 1, x - 1]) -
           (curr_dog[y - 1, x + 1] - curr_dog[y - 1, x - 1])) / 4.0
    # ... dxs, dys tÆ°Æ¡ng tá»±

    # Hessian matrix
    H = [[dxx, dxy, dxs],
         [dxy, dyy, dys],
         [dxs, dys, dss]]

    gradient = [dx, dy, ds]

    # Giáº£i há»‡: HÂ·offset = -gradient
    offset = -np.linalg.solve(H, gradient)

    return offset[1], offset[0], offset[2]  # dy, dx, ds
```

#### 2.5.2. Edge Response Elimination

Loáº¡i bá» keypoints náº±m trÃªn edges (khÃ´ng stable):

Sá»­ dá»¥ng Harris corner detector principle. TÃ­nh tá»· sá»‘ eigenvalues cá»§a Hessian:

```
Tr(H)Â² / Det(H) < threshold
```

Trong Ä‘Ã³:

- `Tr(H) = dxx + dyy` (trace)
- `Det(H) = dxxÂ·dyy - dxyÂ²` (determinant)

```python
# Edge elimination
trace = dxx + dyy
det = dxx * dyy - dxy * dxy

if det <= 0:
    return None

ratio = trace * trace / det
threshold_ratio = ((edge_threshold + 1) ** 2) / edge_threshold

if ratio > threshold_ratio:
    return None  # Edge, loáº¡i bá»
```

**Táº¡i sao?**

- TrÃªn edge: 1 eigenvalue lá»›n, 1 eigenvalue nhá» â†’ ratio lá»›n
- á» corner: 2 eigenvalues tÆ°Æ¡ng Ä‘Æ°Æ¡ng â†’ ratio nhá»

### 2.6. BÆ°á»›c 5: Orientation Assignment

GÃ¡n hÆ°á»›ng dominant cho keypoint Ä‘á»ƒ Ä‘áº¡t rotation invariance.

#### 2.6.1. TÃ­nh Gradient Orientation

Trong vÃ¹ng lÃ¢n cáº­n keypoint (window size = 3Ïƒ):

```
magnitude(x, y) = âˆš((L(x+1,y) - L(x-1,y))Â² + (L(x,y+1) - L(x,y-1))Â²)
orientation(x, y) = atan2(L(x,y+1) - L(x,y-1), L(x+1,y) - L(x-1,y))
```

#### 2.6.2. Orientation Histogram

Táº¡o histogram 36 bins (má»—i bin = 10Â°):

```python
def _compute_keypoint_orientations(self, image, y, x, sigma):
    # Compute gradients trong window
    window_size = int(round(3 * sigma * 1.5))

    # Gradient magnitude vÃ  orientation
    gy = image[2:, :] - image[:-2, :]
    gx = image[:, 2:] - image[:, :-2]

    magnitude = np.sqrt(gx**2 + gy**2)
    orientation = np.arctan2(gy, gx)

    # Táº¡o histogram 36 bins
    num_bins = 36
    hist = np.zeros(num_bins)

    for i in range(magnitude.shape[0]):
        for j in range(magnitude.shape[1]):
            # Gaussian weighting
            weight = magnitude[i, j] * exp(-(dxÂ²+dyÂ²) / (2ÏƒÂ²))

            # ThÃªm vÃ o bin tÆ°Æ¡ng á»©ng
            angle_deg = np.degrees(orientation[i, j]) % 360
            bin_idx = int(angle_deg * num_bins / 360) % num_bins
            hist[bin_idx] += weight

    # Smooth histogram
    hist = np.convolve(hist, [1/3, 1/3, 1/3], mode='same')

    # TÃ¬m peaks (> 80% max)
    orientations = find_peaks(hist, threshold=0.8 * max(hist))

    return orientations
```

**Táº¡i sao cÃ³ thá»ƒ cÃ³ nhiá»u orientations?**

- Má»™t keypoint cÃ³ thá»ƒ cÃ³ nhiá»u dominant directions
- Má»—i orientation táº¡o má»™t keypoint riÃªng biá»‡t
- TÄƒng sá»‘ lÆ°á»£ng keypoints, tÄƒng matching robustness

### 2.7. BÆ°á»›c 6: Descriptor Generation

Táº¡o descriptor 128-chiá»u mÃ´ táº£ vÃ¹ng xung quanh keypoint.

#### 2.7.1. Cáº¥u TrÃºc Descriptor

- Chia vÃ¹ng 16Ã—16 pixels thÃ nh lÆ°á»›i 4Ã—4 = 16 cells
- Má»—i cell cÃ³ histogram 8 bins (8 orientations)
- Tá»•ng: 4 Ã— 4 Ã— 8 = 128 dimensions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4x4 â”‚ 4x4 â”‚ ... â”‚  â† 4Ã—4 grid
â”‚ cellâ”‚ cellâ”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 8-bin histogram â”‚  â† Má»—i cell
â”‚ per cell        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
[128-dim vector]
```

#### 2.7.2. Triá»ƒn Khai

```python
def _compute_descriptor(self, image, y, x, orientation, sigma):
    d = 4  # 4x4 grid
    n = 8  # 8 orientation bins

    window_size = int(round(3 * sigma * d))

    # Compute gradients
    region = image[y-window_size:y+window_size+1,
                  x-window_size:x+window_size+1]

    gy = region[2:, :] - region[:-2, :]
    gx = region[:, 2:] - region[:, :-2]

    magnitude = np.sqrt(gx**2 + gy**2)
    angle = np.arctan2(gy, gx) - orientation  # Rotate theo keypoint
    angle = angle % (2 * np.pi)

    # Initialize descriptor
    descriptor = np.zeros((d, d, n))

    patch_size = 2 * window_size / d

    for i in range(region.shape[0]):
        for j in range(region.shape[1]):
            # Rotate coordinates
            x_rot = cos(orientation) * x_rel - sin(orientation) * y_rel
            y_rot = sin(orientation) * x_rel + cos(orientation) * y_rel

            # Which bin?
            y_bin = (y_rot / patch_size) + d / 2.0
            x_bin = (x_rot / patch_size) + d / 2.0
            angle_bin = (angle[i, j] / (2Ï€)) * n

            # Trilinear interpolation
            # Distribute gradient magnitude to neighboring bins
            # ... (weighted by distance to bin centers)

    # Flatten to 128-D
    descriptor = descriptor.flatten()

    # Normalize
    descriptor = descriptor / np.linalg.norm(descriptor)

    # Clip to 0.2 and renormalize (illumination invariance)
    descriptor = np.clip(descriptor, 0, 0.2)
    descriptor = descriptor / np.linalg.norm(descriptor)

    return descriptor
```

#### 2.7.3. Illumination Invariance

**Normalization**: Loáº¡i bá» áº£nh hÆ°á»Ÿng cá»§a brightness changes

```
descriptor = descriptor / ||descriptor||
```

**Clipping & Re-normalization**: Giáº£m áº£nh hÆ°á»Ÿng cá»§a saturation

```
descriptor = clip(descriptor, 0, 0.2)
descriptor = descriptor / ||descriptor||
```

---

## 3. Feature Matching

### 3.1. CÆ¡ Sá»Ÿ LÃ½ Thuyáº¿t

Matching tÃ¬m correspondences giá»¯a descriptors cá»§a hai áº£nh.

**Má»¥c tiÃªu**: TÃ¬m descriptor trong áº£nh 2 giá»‘ng nháº¥t vá»›i má»—i descriptor trong áº£nh 1.

### 3.2. L2 Distance (Euclidean Distance)

Äá»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a hai descriptors:

```
d(vâ‚, vâ‚‚) = ||vâ‚ - vâ‚‚|| = âˆš(Î£áµ¢(vâ‚áµ¢ - vâ‚‚áµ¢)Â²)
```

**Tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n:**

```
||a - b||Â² = ||a||Â² + ||b||Â² - 2Â·aÂ·b
```

```python
def _compute_distance_matrix(self, desc1, desc2):
    # desc1: N x D
    # desc2: M x D

    # Efficient computation: ||a-b||Â² = ||a||Â² + ||b||Â² - 2(aÂ·b)
    sq_norms1 = np.sum(desc1**2, axis=1, keepdims=True)  # N x 1
    sq_norms2 = np.sum(desc2**2, axis=1, keepdims=True)  # M x 1
    dot_products = np.dot(desc1, desc2.T)  # N x M

    sq_distances = sq_norms1 + sq_norms2.T - 2 * dot_products
    sq_distances = np.maximum(sq_distances, 0)  # Numerical stability

    distances = np.sqrt(sq_distances)

    return distances  # N x M matrix
```

### 3.3. Lowe's Ratio Test

**Váº¥n Ä‘á»**: LÃ m sao biáº¿t má»™t match lÃ  good match?

**Giáº£i phÃ¡p**: So sÃ¡nh vá»›i second-best match:

```
ratio = distance_to_nearest / distance_to_second_nearest
```

**Náº¿u ratio < 0.75**: Match tá»‘t (nearest rÃµ rÃ ng tá»‘t hÆ¡n second-nearest)
**Náº¿u ratio â‰¥ 0.75**: Ambiguous, loáº¡i bá»

```python
def _find_best_matches(self, distances):
    matches = []

    for i in range(distances.shape[0]):
        dists = distances[i]

        # Find two nearest neighbors
        sorted_indices = np.argsort(dists)
        nearest_idx = sorted_indices[0]
        second_nearest_idx = sorted_indices[1]

        nearest_dist = dists[nearest_idx]
        second_nearest_dist = dists[second_nearest_idx]

        # Lowe's ratio test
        if nearest_dist / second_nearest_dist < self.ratio_threshold:
            match = {
                'queryIdx': i,
                'trainIdx': nearest_idx,
                'distance': nearest_dist
            }
            matches.append(match)

    return matches
```

**Táº¡i sao ratio test hiá»‡u quáº£?**

- Loáº¡i bá» matches ambiguous (nhiá»u descriptors giá»‘ng nhau)
- Chá»‰ giá»¯ láº¡i matches distinctive
- Giáº£m false positives

### 3.4. Cross-Check

**ThÃªm má»™t lá»›p validation**: Match pháº£i bidirectional

```
Match(iâ†’j) lÃ  valid náº¿u:
- descriptor[i] trong áº£nh 1 match vá»›i descriptor[j] trong áº£nh 2
- descriptor[j] trong áº£nh 2 cÅ©ng match vá»›i descriptor[i] trong áº£nh 1
```

```python
def _cross_check_matches(self, matches_1to2, matches_2to1):
    # Táº¡o mapping tá»« train â†’ query
    train_to_query = {m['trainIdx']: m['queryIdx'] for m in matches_2to1}

    cross_checked = []
    for match in matches_1to2:
        query_idx = match['queryIdx']
        train_idx = match['trainIdx']

        # Check if reverse match exists and consistent
        if train_idx in train_to_query:
            if train_to_query[train_idx] == query_idx:
                cross_checked.append(match)

    return cross_checked
```

---

## 4. Homography Estimation & RANSAC

### 4.1. Homography Matrix

Homography mÃ´ táº£ phÃ©p biáº¿n Ä‘á»•i projective giá»¯a hai planes.

#### 4.1.1. Äá»‹nh NghÄ©a

```
    [x']       [hâ‚â‚  hâ‚â‚‚  hâ‚â‚ƒ]   [x]
    [y']   =   [hâ‚‚â‚  hâ‚‚â‚‚  hâ‚‚â‚ƒ] Â· [y]
    [w']       [hâ‚ƒâ‚  hâ‚ƒâ‚‚  hâ‚ƒâ‚ƒ]   [1]
```

Sau Ä‘Ã³ normalize:

```
x'_actual = x' / w'
y'_actual = y' / w'
```

#### 4.1.2. Degrees of Freedom

Homography cÃ³ 8 DOF (9 parameters - 1 scale factor):

- **4 point correspondences** cáº§n thiáº¿t Ä‘á»ƒ giáº£i (4 points Ã— 2 equations = 8 equations)

### 4.2. Direct Linear Transform (DLT)

#### 4.2.1. CÆ¡ Sá»Ÿ ToÃ¡n Há»c

Tá»« phÆ°Æ¡ng trÃ¬nh homography:

```
x' = (hâ‚â‚x + hâ‚â‚‚y + hâ‚â‚ƒ) / (hâ‚ƒâ‚x + hâ‚ƒâ‚‚y + hâ‚ƒâ‚ƒ)
y' = (hâ‚‚â‚x + hâ‚‚â‚‚y + hâ‚‚â‚ƒ) / (hâ‚ƒâ‚x + hâ‚ƒâ‚‚y + hâ‚ƒâ‚ƒ)
```

Cross-multiply Ä‘á»ƒ loáº¡i bá» denominator:

```
x'(hâ‚ƒâ‚x + hâ‚ƒâ‚‚y + hâ‚ƒâ‚ƒ) = hâ‚â‚x + hâ‚â‚‚y + hâ‚â‚ƒ
y'(hâ‚ƒâ‚x + hâ‚ƒâ‚‚y + hâ‚ƒâ‚ƒ) = hâ‚‚â‚x + hâ‚‚â‚‚y + hâ‚‚â‚ƒ
```

Rearrange thÃ nh linear equations:

```
-x  -y  -1   0   0   0   xx'  yx'  x'     [hâ‚â‚]
 0   0   0  -x  -y  -1   xy'  yy'  y'  Â·  [hâ‚â‚‚]  = 0
                                           [...]
                                           [hâ‚ƒâ‚ƒ]
```

#### 4.2.2. Triá»ƒn Khai

```python
def _compute_homography_dlt(self, src_pts, dst_pts):
    n = len(src_pts)

    # Normalize points (numerical stability)
    src_norm, T_src = self._normalize_points(src_pts)
    dst_norm, T_dst = self._normalize_points(dst_pts)

    # Build matrix A
    A = []
    for i in range(n):
        x, y = src_norm[i]
        x_p, y_p = dst_norm[i]

        # Two rows per correspondence
        A.append([-x, -y, -1,  0,  0,  0,  x*x_p,  y*x_p,  x_p])
        A.append([ 0,  0,  0, -x, -y, -1,  x*y_p,  y*y_p,  y_p])

    A = np.array(A)

    # Solve using SVD: AÂ·h = 0
    # Solution is last column of V (smallest singular value)
    U, S, Vt = np.linalg.svd(A)
    H = Vt[-1].reshape(3, 3)

    # Denormalize
    H = np.linalg.inv(T_dst) @ H @ T_src

    # Normalize so H[2,2] = 1
    H = H / H[2, 2]

    return H
```

#### 4.2.3. Point Normalization

**Táº¡i sao cáº§n normalize?**

- Coordinates cÃ³ thá»ƒ ráº¥t lá»›n (e.g., 1920Ã—1080)
- GÃ¢y ill-conditioned matrix
- SVD khÃ´ng stable

**CÃ¡ch normalize:**

1. Translate sao cho centroid á»Ÿ origin
2. Scale sao cho average distance = âˆš2

```python
def _normalize_points(self, points):
    # Compute centroid
    centroid = np.mean(points, axis=0)

    # Center points
    points_centered = points - centroid

    # Compute average distance
    avg_dist = np.mean(np.sqrt(np.sum(points_centered**2, axis=1)))

    # Scale factor
    scale = np.sqrt(2) / avg_dist

    # Transformation matrix
    T = [[scale,     0,  -scale * centroid[0]],
         [    0, scale,  -scale * centroid[1]],
         [    0,     0,                      1]]

    return points_normalized, T
```

### 4.3. RANSAC Algorithm

**RANdom SAmple Consensus** - Robust estimation vá»›i outliers.

#### 4.3.1. Váº¥n Äá»

Matches cÃ³ thá»ƒ chá»©a outliers (false matches):

- Wrong correspondences
- Moving objects
- Repetitive patterns

â†’ Cáº§n thuáº­t toÃ¡n robust!

#### 4.3.2. Ã TÆ°á»Ÿng RANSAC

```
Repeat N times:
    1. Sample 4 random points
    2. Compute homography H from 4 points
    3. Count inliers (points with reprojection error < threshold)
    4. Keep H with most inliers

Refine H using all inliers
```

#### 4.3.3. Triá»ƒn Khai

```python
def find_homography(self, src_points, dst_points):
    best_H = None
    best_inliers = None
    best_num_inliers = 0

    n_points = len(src_points)

    for iteration in range(self.max_iters):
        # 1. Random sample 4 points
        indices = np.random.choice(n_points, 4, replace=False)
        src_sample = src_points[indices]
        dst_sample = dst_points[indices]

        # 2. Compute homography
        H = self._compute_homography_4pts(src_sample, dst_sample)

        if H is None:
            continue

        # 3. Count inliers
        inliers = self._get_inliers(src_points, dst_points, H)
        num_inliers = np.sum(inliers)

        # 4. Update best
        if num_inliers > best_num_inliers:
            best_num_inliers = num_inliers
            best_inliers = inliers
            best_H = H

            # Adaptive termination
            inlier_ratio = num_inliers / n_points
            n_iters_needed = log(1 - confidence) / log(1 - inlier_ratio^4)
            if iteration > n_iters_needed:
                break

    # Refine vá»›i all inliers
    if best_H is not None:
        inlier_src = src_points[best_inliers]
        inlier_dst = dst_points[best_inliers]
        best_H = self._compute_homography_dlt(inlier_src, inlier_dst)

    return best_H, best_inliers
```

#### 4.3.4. Inlier Detection

Reprojection error:

```
error = ||dst_point - HÂ·src_point||
```

```python
def _get_inliers(self, src_pts, dst_pts, H):
    # Transform source points
    src_homogeneous = np.hstack([src_pts, np.ones((len(src_pts), 1))])
    dst_projected = (H @ src_homogeneous.T).T

    # Convert from homogeneous
    dst_projected = dst_projected[:, :2] / dst_projected[:, 2:3]

    # Compute errors
    errors = np.sqrt(np.sum((dst_pts - dst_projected)**2, axis=1))

    # Inliers: error < threshold
    inliers = errors < self.ransac_reproj_threshold

    return inliers
```

#### 4.3.5. Sá»‘ LÆ°á»£ng Iterations Cáº§n Thiáº¿t

Probability of success:

```
P(success) = 1 - (1 - p^s)^N
```

Trong Ä‘Ã³:

- `p`: Inlier ratio (e.g., 0.5)
- `s`: Sample size (4 for homography)
- `N`: Number of iterations

Giáº£i cho N vá»›i confidence = 0.995:

```
N = log(1 - confidence) / log(1 - p^s)
```

VÃ­ dá»¥ vá»›i p=0.5, s=4, confidence=0.995:

```
N = log(0.005) / log(1 - 0.5^4) â‰ˆ 35 iterations
```

---

## 5. Image Warping

### 5.1. Perspective Transformation

Apply homography H Ä‘á»ƒ warp image.

#### 5.1.1. Forward vs Backward Warping

**Forward Warping** (khÃ´ng dÃ¹ng):

```
For each pixel (x,y) in source:
    Compute (x',y') = H Â· (x,y)
    Set output[x',y'] = input[x,y]
```

âŒ Problem: Holes trong output (khÃ´ng phá»§ kÃ­n)

**Backward Warping** (dÃ¹ng):

```
For each pixel (x',y') in output:
    Compute (x,y) = Hâ»Â¹ Â· (x',y')
    Set output[x',y'] = interpolate(input, x, y)
```

âœ… No holes, má»—i output pixel cÃ³ giÃ¡ trá»‹

#### 5.1.2. Triá»ƒn Khai

```python
def warp_perspective(image, H, output_shape):
    h, w = output_shape

    # Inverse homography
    H_inv = np.linalg.inv(H)

    # Create coordinate grid
    y_coords, x_coords = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')
    coords = np.stack([x_coords.flatten(),
                       y_coords.flatten(),
                       np.ones(h * w)], axis=1)

    # Apply inverse homography
    src_coords = (H_inv @ coords.T).T
    src_coords = src_coords[:, :2] / src_coords[:, 2:3]

    # Reshape
    src_x = src_coords[:, 0].reshape(h, w)
    src_y = src_coords[:, 1].reshape(h, w)

    # Bilinear interpolation
    output = bilinear_interpolate(image, src_x, src_y)

    return output
```

### 5.2. Bilinear Interpolation

Khi (x, y) khÃ´ng pháº£i integer, interpolate tá»« 4 neighboring pixels.

#### 5.2.1. CÃ´ng Thá»©c

Cho pixel táº¡i (x, y) vá»›i x = xâ‚€ + fx, y = yâ‚€ + fy:

```
I(x, y) = (1-fx)(1-fy)Â·I(xâ‚€,yâ‚€) + fx(1-fy)Â·I(xâ‚,yâ‚€) +
          (1-fx)fyÂ·I(xâ‚€,yâ‚) + fxÂ·fyÂ·I(xâ‚,yâ‚)
```

Trong Ä‘Ã³:

- `(xâ‚€, yâ‚€)`: Top-left pixel
- `fx, fy`: Fractional parts

```
    (xâ‚€,yâ‚€)â”€â”€â”€â”€â”€â”€â”€â”€(xâ‚,yâ‚€)
        â”‚             â”‚
        â”‚    (x,y)    â”‚
        â”‚      Â·      â”‚
        â”‚             â”‚
    (xâ‚€,yâ‚)â”€â”€â”€â”€â”€â”€â”€â”€(xâ‚,yâ‚)
```

#### 5.2.2. Triá»ƒn Khai

```python
def bilinear_interpolate(image, x, y):
    h, w = image.shape[:2]

    # Integer coordinates
    x0 = np.floor(x).astype(int)
    x1 = x0 + 1
    y0 = np.floor(y).astype(int)
    y1 = y0 + 1

    # Clip to boundaries
    x0 = np.clip(x0, 0, w - 1)
    x1 = np.clip(x1, 0, w - 1)
    y0 = np.clip(y0, 0, h - 1)
    y1 = np.clip(y1, 0, h - 1)

    # Fractional parts
    fx = x - x0
    fy = y - y0

    # Bounds mask
    mask = (x >= 0) & (x < w) & (y >= 0) & (y < h)

    # Interpolate for each channel
    if len(image.shape) == 3:
        output = np.zeros((y.shape[0], y.shape[1], channels))
        for c in range(channels):
            I00 = image[y0, x0, c]
            I01 = image[y1, x0, c]
            I10 = image[y0, x1, c]
            I11 = image[y1, x1, c]

            w00 = (1 - fx) * (1 - fy)
            w01 = (1 - fx) * fy
            w10 = fx * (1 - fy)
            w11 = fx * fy

            output[:, :, c] = (w00*I00 + w01*I01 + w10*I10 + w11*I11) * mask

    return output
```

---

## 6. Weighted Blending

### 6.1. Váº¥n Äá» Cáº§n Giáº£i Quyáº¿t

Khi ghÃ©p áº£nh, vÃ¹ng overlap cÃ³ thá»ƒ cÃ³:

- **Seams rÃµ rÃ ng** (hard edges)
- **Differences vá» exposure/illumination**
- **Vignetting** (darkening á»Ÿ gÃ³c áº£nh)

â†’ Cáº§n blending mÆ°á»£t mÃ !

### 6.2. Alpha Blending vá»›i Linear Gradient

#### 6.2.1. Ã TÆ°á»Ÿng

Trong vÃ¹ng overlap, blend hai áº£nh vá»›i weights thay Ä‘á»•i dáº§n:

```
Result = Î±Â·Image1 + (1-Î±)Â·Image2
```

Trong Ä‘Ã³ Î± thay Ä‘á»•i tá»« 1 â†’ 0 trong vÃ¹ng overlap.

#### 6.2.2. Mask Creation

```python
def _create_mask(self, canvas1, canvas2, img1, x_offset, version='left'):
    h, w = canvas1.shape[:2]

    # Find valid regions
    mask1_valid = np.any(canvas1 > 0, axis=2)
    mask2_valid = np.any(canvas2 > 0, axis=2)

    # Overlap region
    overlap = mask1_valid & mask2_valid

    # Find overlap boundaries
    overlap_cols = np.where(np.any(overlap, axis=0))[0]
    overlap_start = overlap_cols[0]
    overlap_end = overlap_cols[-1]
    overlap_width = overlap_end - overlap_start

    # Create mask
    mask = np.zeros((h, w))

    if version == 'left':
        # Left image: 1.0 â†’ 0.0
        mask[mask1_valid] = 1.0

        for col in range(overlap_start, overlap_end + 1):
            alpha = 1.0 - (col - overlap_start) / overlap_width
            mask[overlap[:, col], col] = alpha

    else:
        # Right image: 0.0 â†’ 1.0
        mask[mask2_valid] = 1.0

        for col in range(overlap_start, overlap_end + 1):
            alpha = (col - overlap_start) / overlap_width
            mask[overlap[:, col], col] = alpha

    return mask
```

### 6.3. Gaussian Smoothing

LÃ m mÆ°á»£t mask vá»›i Gaussian filter Ä‘á»ƒ trÃ¡nh banding artifacts:

```python
mask = gaussian_filter(mask, sigma=smoothing_window/6.0)
```

### 6.4. Final Blending

```python
def blend_images(self, img1, img2, H):
    # Warp img2
    img2_warped = warp_perspective(img2, H_translated, canvas_size)

    # Create masks
    mask1 = self._create_mask(..., version='left')
    mask2 = self._create_mask(..., version='right')

    # Blend
    blended = canvas1 * mask1 + img2_warped * mask2

    # Crop black borders
    blended = self._crop_black_borders(blended)

    return blended
```

### 6.5. Visualization cá»§a Blending Process

```
Image 1 Mask:          Image 2 Mask:
â”‚ 1.0                  â”‚ 0.0
â”‚ 1.0                  â”‚ 0.0
â”‚ 1.0â†’0.5â†’0.0         â”‚ 0.0â†’0.5â†’1.0
â”‚     overlap          â”‚     overlap
â”‚                      â”‚                1.0
â”‚                      â”‚                1.0

Combined:
â”‚ Image1 only
â”‚ Image1 only
â”‚ Smooth blend region
â”‚             Image2 only
â”‚             Image2 only
```

---

## 7. Tham Kháº£o

### Papers

1. **Lowe, D. G. (2004)**  
   _"Distinctive Image Features from Scale-Invariant Keypoints"_  
   International Journal of Computer Vision, 60(2), 91-110  
   [Link](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)

2. **Brown, M., & Lowe, D. G. (2007)**  
   _"Automatic Panoramic Image Stitching using Invariant Features"_  
   International Journal of Computer Vision, 74(1), 59-73

3. **Hartley, R., & Zisserman, A. (2004)**  
   _"Multiple View Geometry in Computer Vision"_  
   Cambridge University Press (Chapter 4: Homography Estimation)

4. **Fischler, M. A., & Bolles, R. C. (1981)**  
   _"Random Sample Consensus: A Paradigm for Model Fitting"_  
   Communications of the ACM, 24(6), 381-395

### Courses

1. **First Principles of Computer Vision - Shree K. Nayar**  
   Columbia University  
   [https://fpcv.cs.columbia.edu/](https://fpcv.cs.columbia.edu/)

2. **Computer Vision - Andrew Ng**  
   Stanford University CS231n

### Tutorials

1. **OpenCV SIFT Tutorial**  
   [https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html](https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html)

2. **Image Stitching with OpenCV**  
   [https://learnopencv.com/image-alignment-feature-based-using-opencv-c-python/](https://learnopencv.com/image-alignment-feature-based-using-opencv-c-python/)

---

## ğŸ“ LiÃªn Há»‡ & ÄÃ³ng GÃ³p

Náº¿u báº¡n cÃ³ cÃ¢u há»i hoáº·c muá»‘n Ä‘Ã³ng gÃ³p cáº£i thiá»‡n tÃ i liá»‡u nÃ y, vui lÃ²ng táº¡o issue hoáº·c pull request!

**Happy Learning!** ğŸ“âœ¨
